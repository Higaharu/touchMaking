# ジェスチャーで物質を操るMRアプリ

このプロジェクトは、ブラウザベースでジェスチャー操作によって仮想的な物質（砂、水、粘土、光の粒子）の形状をインタラクティブに変化させることができるMR（Mixed Reality）アプリケーションです。PCのWebカメラを通じて手の動きを認識し、それに応じて画面上の粒子が反応します。

## 特徴

- **複数の物質シミュレーション**: 異なる物理特性を持つ4種類の物質（砂、水、粘土、光の粒子）を体験できます
- **ジェスチャー認識**: MediaPipe Handsを使用してWebカメラでリアルタイムにハンドジェスチャーを検出
- **直感的な操作**: 手を握る、指す、開く、振るなどのジェスチャーで物質を変形
- **音響フィードバック**: ジェスチャーと物質に応じた聴覚的フィードバック
- **MR体験**: Webカメラの映像と物質シミュレーションを合成し、拡張された現実感を提供

## 使用技術

- **フロントエンド**: HTML, CSS, JavaScript
- **ハンドトラッキング**: MediaPipe Hands
- **グラフィックス**: Canvas API
- **音響**: Web Audio API

## 使い方

1. ブラウザ（Chrome推奨）でindex.htmlを開きます
2. Webカメラへのアクセスを許可します
3. 画面上部のボタンから操作したい物質を選択します
4. 以下のジェスチャーを使って物質を操作します：
   - 👋 **手を振る**: 物質をかき混ぜる
   - ✊ **握る**: 物質をつかむ
   - 👆 **指す**: 物質を押す
   - 🖐 **手のひらを開く**: 物質を広げる

## 必要条件

- Webカメラを搭載したコンピュータ
- 最新版のウェブブラウザ（Google Chrome推奨）
- JavaScript有効

## デバッグモード

キーボードの「d」キーを押すと、デバッグ情報（ハンド検出状態、現在のジェスチャー、選択中の物質、粒子数）を表示するパネルの表示/非表示を切り替えることができます。

## 今後の開発予定

- パフォーマンスの最適化
- スマートフォン／タブレット対応
- 複数の手の同時認識
- より豊かな物理シミュレーション
- 物質の保存・共有機能

## HCI研究としての視点

このプロジェクトは以下のHCI研究トピックに関連しています：

- 身体的なインタラクションとデジタル表現の融合
- ジェスチャーベースのインターフェースのデザイン
- 多感覚フィードバックによるユーザー体験の向上
- ブラウザベースのMR/AR体験の可能性と限界

## ライセンス

このプロジェクトは[MITライセンス](https://opensource.org/licenses/MIT)の下で公開されています。 